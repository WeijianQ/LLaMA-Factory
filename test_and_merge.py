#!/usr/bin/env python3
"""
Test and merge embedding logic, then save the converted model.

Test flow:
1. Load Qwen2_5_MemoryForCausalLMForFreezeTraining model
2. Extract state_dict and merge special embeddings into normal embed_tokens
3. Initialize Qwen2_5_MemoryForCausalLM with merged weights
4. Verify embeddings and parameters are correctly merged
5. Forward both models with same input to verify logits match
6. Save the converted normal model

Usage:
    python test_and_merge.py --model_path <model_path> [--save_path <save_path>] [--pkl_path <pkl_path>]
"""

import argparse
import pickle
import torch
import os
from transformers import AutoTokenizer, AutoConfig, AutoProcessor
from src.llamafactory.hf_memory_qwen25.modeling_qwen2_5_memory import (
    Qwen2_5_MemoryForCausalLM,
    Qwen2_5_MemoryForCausalLMForFreezeTraining
)
from src.llamafactory.hf_memory_qwen25.configuration_qwen2_5_memory import Qwen2_5_MemoryConfig
from copy import deepcopy


def convert_state_dict_from_freeze_to_normal(freeze_state_dict, config):
    """
    Convert state dict from freeze training model to normal model.
    """
    # copy
    normal_state_dict = deepcopy(freeze_state_dict)

    # Pop special layers (not needed in normal model)
    special_embed_tokens_weight = normal_state_dict.pop('special_embed_tokens.weight')
    normal_state_dict.pop('special_lm_head.weight', None)  # Tied to special_embed_tokens
    normal_state_dict.pop('id2override', None)  # Only used in freeze model

    # Pop lm_head.weight so it will be tied to embed_tokens.weight
    normal_state_dict.pop('lm_head.weight')

    # Merge special embeddings into embed_tokens
    current_embed_tokens_weight = normal_state_dict['model.embed_tokens.weight']

    for idx, token_id in enumerate(config.special_token_ids):
        current_embed_tokens_weight[token_id] = special_embed_tokens_weight[idx]
        print(f"Merged special token {token_id} into embed_tokens.weight")
    print(f"Merged {len(config.special_token_ids)} special tokens into embed_tokens.weight")

    return normal_state_dict


def main():
    parser = argparse.ArgumentParser(description="Test and merge freeze training model to normal model")
    parser.add_argument("--model_path", type=str, required=True,
                        help="Path to the freeze training model")
    parser.add_argument("--save_path", type=str, default=None,
                        help="Path to save the converted model. If not specified, will use model_path + '_converted'")
    parser.add_argument("--pkl_path", type=str,
                        default="batch_sample_webshop_train_keep_action_with_cm_policy_only.pkl",
                        help="Path to the sample data pickle file for testing")

    args = parser.parse_args()
    if not os.path.exists(args.pkl_path):
        print(f"Error: Sample data file not found at {args.pkl_path}")
        exit(1)
    # Determine save path
    if args.save_path is None:
        args.save_path = args.model_path.rstrip('/') + "_converted"
        print(f"No save path specified, using default save path: {args.save_path}")

    print("=" * 80)
    print("Test and Merge: Freeze Training Model to Normal Model")
    print("=" * 80)
    print(f"Model path: {args.model_path}")
    print(f"Save path: {args.save_path}")
    print(f"Sample data: {args.pkl_path}")

    # Load config
    print(f"\nLoading config from {args.model_path}...")
    config = Qwen2_5_MemoryConfig.from_pretrained(args.model_path, trust_remote_code=True)

    # Step 1: Load freeze training model
    print(f"\n[Step 1] Loading Qwen2_5_MemoryForCausalLMForFreezeTraining...")
    freeze_model = Qwen2_5_MemoryForCausalLMForFreezeTraining.from_pretrained(
        args.model_path,
        config=config,
        device_map="cpu",
        trust_remote_code=True,
        torch_dtype=torch.bfloat16,
    )

    # Re-establish weight tying (from_pretrained may have broken it)
    freeze_model.special_lm_head.weight = freeze_model.special_embed_tokens.weight
    print(f"  Re-established weight tying: special_lm_head.weight = special_embed_tokens.weight")

    freeze_model.eval()

    # Step 2: Extract state_dict and convert
    print(f"\n[Step 2] Extracting state_dict and converting to normal model format...")
    freeze_state_dict = freeze_model.state_dict()

    # Convert to normal model format
    merged_state_dict = convert_state_dict_from_freeze_to_normal(freeze_state_dict, config)

    # Assert lm_head.weight is NOT in merged_state_dict (it should be tied)
    assert 'lm_head.weight' not in merged_state_dict, "ERROR: lm_head.weight should not be in merged state_dict!"
    print(f"  ✓ Verified: lm_head.weight is NOT in state_dict (will be tied to embed_tokens.weight)")

    # Step 3: Initialize normal model with merged weights
    print(f"\n[Step 3] Loading Qwen2_5_MemoryForCausalLM with merged weights...")
    clean_config = AutoConfig.from_pretrained("WeijianQi1999/Qwen25-1p5B-Memory", trust_remote_code=True)
    normal_model = Qwen2_5_MemoryForCausalLM.from_pretrained(
        args.model_path,
        config=clean_config,
        device_map="cpu",
        trust_remote_code=True,
        torch_dtype=torch.bfloat16,
    )

    # Load merged state dict
    missing_keys, unexpected_keys = normal_model.load_state_dict(merged_state_dict, strict=False)
    print(f"  Missing keys: {missing_keys}")
    print(f"  Unexpected keys: {unexpected_keys}")
    normal_model.eval()

    # Verify weight tying
    print(f"\n  Verifying weight tying...")
    is_tied = normal_model.lm_head.weight is normal_model.model.embed_tokens.weight
    print(f"    lm_head.weight is embed_tokens.weight: {is_tied}")
    assert is_tied, "ERROR: lm_head and embed_tokens should be tied!"
    print(f"    ✓ Verified: Weight tying is correctly established")

    # Verify embeddings were merged correctly
    print(f"\n  Verifying merged embeddings...")
    special_token_ids = torch.tensor(config.special_token_ids)
    for idx, token_id in enumerate(special_token_ids):
        freeze_emb = freeze_model.special_embed_tokens.weight[idx]
        normal_emb = normal_model.model.embed_tokens.weight[token_id]
        diff = (freeze_emb - normal_emb).abs().max().item()
        print(f"    Token {token_id}: max diff = {diff:.6e}")
        assert diff < 1e-3, f"ERROR: Embeddings don't match for special token {token_id}!"  # Relaxed for bf16
    print(f"    ✓ Verified: All special token embeddings merged correctly")

    # Step 4: Load sample data and test forward pass
    if os.path.exists(args.pkl_path):
        print(f"\n[Step 4] Loading sample data from {args.pkl_path}...")
        with open(args.pkl_path, "rb") as f:
            sample_data = pickle.load(f)

        # Take first sample only
        input_ids = sample_data["input_ids"][:1].to(torch.long)
        attention_mask = sample_data["attention_mask"][:1].to(torch.long)
        memory_input_ids = None
        memory_attention_mask = None

        print(f"  Input shapes:")
        print(f"    input_ids: {input_ids.shape}")
        print(f"    attention_mask: {attention_mask.shape}")

        # Check for special tokens in input
        special_token_mask = torch.isin(input_ids, special_token_ids)
        num_special_tokens = special_token_mask.sum().item()
        print(f"  Number of special tokens in input: {num_special_tokens}")

        # Step 5: Verify parameter consistency
        print(f"\n[Step 5] Verifying parameter consistency...")
        normal_embed_tokens_weight = normal_model.model.embed_tokens.weight
        freeze_embed_tokens_weight = freeze_model.model.embed_tokens.weight
        freeze_special_embed_tokens_weight = freeze_model.special_embed_tokens.weight
        special_token_ids_map = {token_id.item(): idx for idx, token_id in enumerate(special_token_ids)}

        # Sample check (not all tokens to save time)
        sample_token_ids = list(range(0, config.vocab_size, config.vocab_size // 100))
        for token_id in sample_token_ids:
            if token_id in special_token_ids_map:
                expected = freeze_special_embed_tokens_weight[special_token_ids_map[token_id]]
                actual = normal_embed_tokens_weight[token_id]
                assert torch.allclose(actual, expected, rtol=1e-2, atol=1e-3), \
                    f"Embedding mismatch for special token {token_id}"
            else:
                expected = freeze_embed_tokens_weight[token_id]
                actual = normal_embed_tokens_weight[token_id]
                assert torch.allclose(actual, expected, rtol=1e-2, atol=1e-3), \
                    f"Embedding mismatch for token {token_id}"
        print(f"  ✓ Verified: Sample embed_tokens weights match")

        # Step 6: Forward both models
        print(f"\n[Step 6] Running forward pass on both models...")

        # Move inputs to same device as models
        device = next(freeze_model.parameters()).device
        input_ids = input_ids.to(device)
        attention_mask = attention_mask.to(device)

        with torch.no_grad():
            # Forward freeze model
            print("  Forward freeze_model...")
            freeze_outputs = freeze_model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                memory_input_ids=memory_input_ids,
                memory_attention_mask=memory_attention_mask,
            )
            freeze_logits = freeze_outputs.logits

            # Forward normal model
            print("  Forward normal_model...")
            normal_outputs = normal_model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                memory_input_ids=memory_input_ids,
                memory_attention_mask=memory_attention_mask,
            )
            normal_logits = normal_outputs.logits

        print(f"  Freeze model logits shape: {freeze_logits.shape}")
        print(f"  Normal model logits shape: {normal_logits.shape}")

        # Step 7: Compare logits
        print(f"\n[Step 7] Comparing logits...")

        # Compute differences
        abs_diff = (freeze_logits - normal_logits).abs()
        max_diff = abs_diff.max().item()
        mean_diff = abs_diff.mean().item()

        print(f"  Max absolute difference: {max_diff:.6e}")
        print(f"  Mean absolute difference: {mean_diff:.6e}")

        # Check if logits are close (relaxed tolerance for bf16)
        tolerance = 1e-2
        are_close = torch.allclose(freeze_logits, normal_logits, rtol=1e-2, atol=tolerance)

        print(f"\n{'='*80}")
        if are_close:
            print("✅ PASS: Logits are identical (within tolerance)!")
            print(f"   Tolerance: {tolerance:.6e}")
        else:
            print("⚠️  WARNING: Logits differ slightly (expected with bf16)")
            print(f"   Max diff: {max_diff:.6e}")
            print(f"   Mean diff: {mean_diff:.6e}")

            # Print top differences
            print(f"\n  Top 5 differences:")
            flat_diff = abs_diff.view(-1)
            top_k = min(5, flat_diff.numel())
            top_diffs, top_indices = torch.topk(flat_diff, top_k)
            for i, (diff, idx) in enumerate(zip(top_diffs, top_indices)):
                pos = idx.item()
                batch_idx = pos // (freeze_logits.shape[1] * freeze_logits.shape[2])
                seq_idx = (pos % (freeze_logits.shape[1] * freeze_logits.shape[2])) // freeze_logits.shape[2]
                vocab_idx = pos % freeze_logits.shape[2]
                print(f"    {i+1}. Position [batch={batch_idx}, seq={seq_idx}, vocab={vocab_idx}]: "
                      f"diff={diff.item():.6e}, "
                      f"freeze={freeze_logits[batch_idx, seq_idx, vocab_idx].item():.6f}, "
                      f"normal={normal_logits[batch_idx, seq_idx, vocab_idx].item():.6f}")
        print("=" * 80)
    else:
        print(f"\n⚠️  WARNING: Sample data file not found at {args.pkl_path}")
        print("Skipping forward pass verification...")

    # Step 8: Save the converted model
    print(f"\n[Step 8] Saving converted model to {args.save_path}...")
    os.makedirs(args.save_path, exist_ok=True)

    # Save model
    normal_model.save_pretrained(args.save_path, max_shard_size="5GB")
    # save config, save tokenizer, save processor
    normal_model.config.save_pretrained(args.save_path)
    tokenizer = AutoTokenizer.from_pretrained("WeijianQi1999/Qwen25-1p5B-Memory", trust_remote_code=True)
    tokenizer.save_pretrained(args.save_path)
    processor = AutoProcessor.from_pretrained("WeijianQi1999/Qwen25-1p5B-Memory", trust_remote_code=True)
    processor.save_pretrained(args.save_path)

    # Save tokenizer and config
    try:
        tokenizer = AutoTokenizer.from_pretrained(args.model_path, trust_remote_code=True)
        tokenizer.save_pretrained(args.save_path)
        print("  ✓ Tokenizer saved")
    except Exception as e:
        print(f"  ⚠️  Warning: Could not save tokenizer: {e}")

    config.save_pretrained(args.save_path)
    print("  ✓ Config saved")
    print("  ✓ Model saved")

    print(f"\n{'='*80}")
    print("✅ Conversion complete!")
    print(f"Converted model saved to: {args.save_path}")
    print("=" * 80)


if __name__ == "__main__":
    main()
